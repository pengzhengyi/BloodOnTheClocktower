{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install beautifulsoup4 ipywidgets requests rich tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import requests\n",
    "from typing import Any\n",
    "from urllib.parse import urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "from rich.console import Console\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "console = Console()\n",
    "\n",
    "def get_root_dir() -> str:\n",
    "    dirpath = os.getcwd()\n",
    "    while os.path.basename(dirpath) != 'BloodOnTheClocktower':\n",
    "        dirpath = os.path.dirname(dirpath)\n",
    "    return dirpath\n",
    "\n",
    "\n",
    "def get_edition_dir() -> str:\n",
    "    return os.path.join(get_root_dir(), 'content', 'editions')\n",
    "\n",
    "\n",
    "main_page_url = \"https://wiki.bloodontheclocktower.com/Main_Page\"\n",
    "\n",
    "def get_edition_links() -> list[str]:\n",
    "    main_page = requests.get(main_page_url).text \n",
    "    main_page_soup = BeautifulSoup(main_page)\n",
    "\n",
    "    edition_header = main_page_soup.find(\"h2\", string= \"Characters By Edition\")\n",
    "    edition_links_div = edition_header.find_next_sibling('div', class_=\"row\")\n",
    "    edition_hrefs = edition_links_div.find_all(\"a\", class_=lambda class_: class_ is None or class_.lower() != 'internal')\n",
    "    return [urljoin(main_page_url, edition_href['href']) for edition_href in edition_hrefs]\n",
    "\n",
    "\n",
    "def scrape_edition_page(url: str) -> dict[str, Any]:\n",
    "    page = requests.get(url).text\n",
    "    soup = BeautifulSoup(page)\n",
    "\n",
    "    synopsis_title = soup.find(id=\"Synopsis\")\n",
    "    synopsis_div = next(parent for parent in synopsis_title.parents if parent.name == 'div')\n",
    "    synopsis = '\\n'.join(paragraph.get_text().strip() for paragraph in synopsis_div.find_all('p'))\n",
    "\n",
    "    main_content_div = synopsis_div.find_next_sibling('div')\n",
    "    characters: dict[str, list[str]] = dict()\n",
    "    for characters_group_header in main_content_div.find_all(\"h3\"):\n",
    "        character_groupname = characters_group_header.get_text().strip()\n",
    "        character_groupname_lower = character_groupname.lower()\n",
    "        character_group_ul = soup.find(id=character_groupname).parent.find_next_sibling('ul')\n",
    "        character_names = [character.get_text().strip() for character in character_group_ul.find_all(\"h4\")]\n",
    "        characters[character_groupname_lower] = character_names\n",
    "\n",
    "    table_of_content = soup.find(id=\"toc\")\n",
    "    description_paragraphs = list(reversed(table_of_content.find_previous_siblings(\"p\")))\n",
    "    description = '\\n'.join(paragraph.get_text().strip() for paragraph in description_paragraphs).strip()\n",
    "\n",
    "    try:\n",
    "        difficulty = description_paragraphs[1].get_text().split('.', maxsplit=1)[0].strip()\n",
    "    except Exception:\n",
    "        console.print_exception(show_locals=True)\n",
    "        difficulty = 'Not Specified'\n",
    "\n",
    "    guide: dict[str, str] = dict()\n",
    "    try:\n",
    "        good_player_guide_index = description.index(\"Good players\")\n",
    "        evil_player_guide_index = description.index(\"Evil players\")\n",
    "        guide[\"good players\"] = description[good_player_guide_index:evil_player_guide_index].strip()\n",
    "        guide[\"evil players\"] = description[evil_player_guide_index:].strip()\n",
    "    except Exception:\n",
    "        console.print_exception(show_locals=True)\n",
    "        \n",
    "\n",
    "    data =  {\n",
    "        \"name\": soup.h1.string,\n",
    "        \"url\": url,\n",
    "        \"synopsis\": synopsis,\n",
    "        \"characters\": characters,\n",
    "        \"description\": description,\n",
    "        \"difficulty\": difficulty,\n",
    "        \"guide\": guide\n",
    "    }\n",
    "    console.print(data)\n",
    "\n",
    "    return data \n",
    "\n",
    "\n",
    "def write_editions(edition_folder: str) -> None:\n",
    "    edition_links = get_edition_links()\n",
    "    for edition_link in tqdm(edition_links):\n",
    "        data = scrape_edition_page(edition_link)\n",
    "        filepath = os.path.join(edition_folder, f'{data[\"name\"]}.json')\n",
    "        with open(filepath, 'w') as edition_filewriter:\n",
    "            json.dump(data, edition_filewriter, indent=4, sort_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_editions(get_edition_dir())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c3ecfbb2e518759e8150e3a4e02d9374d8d11351137435699613669ebcf0f527"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
